{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import time as ti\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import helper\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "path_cifar = 'cifar-10-batches-py' #data donwloaded from  https://www.cs.toronto.edu/~kriz/cifar.html - CIFAR-10 python version\n",
    "batch_id = 5\n",
    "sample_id = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "\n",
      "Example of Image 7:\n",
      "Image - Min Value: 20 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 3 Name: cat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e2604be10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "helper.display_stats(path_cifar, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 9 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e22790278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_id = 3\n",
    "sample_id = 5\n",
    "helper.display_stats(path_cifar, batch_id, sample_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is divided in 5 batches from the CIFAR-10, consisting of 10 different classes listed below:\n",
    "\n",
    "* airplane\n",
    "* automobile\n",
    "* bird \n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(labels):\n",
    "    one_hot = np.zeros((len(labels), 10))\n",
    "    for label, array in zip(labels, one_hot):\n",
    "        array[label] = 1\n",
    "    return one_hot\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \n",
    "    array = np.array(x)\n",
    "    max_image = array.max()\n",
    "    min_image = array.min()\n",
    "    norm = (array - min_image)/(max_image - min_image)\n",
    "    return norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(path_cifar, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_pool(x, outputs, conv_str, conv_ksize, pool_ksi, pool_str):\n",
    "    _, _, _, deph = x.shape\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], deph.value, outputs], \n",
    "                                             stddev = 0.015, dtype=tf.float32))\n",
    "    bias = tf.Variable(tf.zeros([outputs]))\n",
    "    conv_strides = [1, conv_str[0], conv_str[1], 1]\n",
    "    conv_ksize = [1, conv_ksize[0], conv_ksize[1], 1]\n",
    "    pool_ksize = [1, pool_ksi[0], pool_ksi[1], 1]\n",
    "    pool_stride = [1, pool_str[0], pool_str[1], 1]\n",
    "    \n",
    "    conv2d = tf.nn.conv2d(x, weight, conv_strides, padding = 'SAME')\n",
    "    conv2d_b = tf.add(conv2d, bias)\n",
    "    conv2d_b_act = tf.nn.relu(conv2d_b)\n",
    "    conv_pool = tf.nn.max_pool(conv2d_b_act, pool_ksize, pool_stride, padding= 'SAME')\n",
    "    return conv_pool\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    return tf.contrib.layers.flatten(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(x_, outputs):\n",
    "    \n",
    "    #outputs is the number of outputs of the new tensor\n",
    "    #return a 2-D tensor where the second dimension is outputs\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_, num_outputs=outputs, activation_fn=tf.nn.relu)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_out(x_, outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_, num_outputs=outputs, activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Network\n",
    "\n",
    "\n",
    "## Optimization\n",
    "\n",
    "#### Simply, training neural net\n",
    "* x for images\n",
    "* y for labels\n",
    "* keep_prob for dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_net(sess, optimizer, keep_probability, xs_batch, ys_batch):\n",
    "    #training session\n",
    "    #batch x: images\n",
    "    #batch y: labels\n",
    "    \n",
    "    sess.run(optimizer, feed_dict={x:xs_batch, y: ys_batch, keep_prob:keep_probability})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(sess, xs_batch, ys_batch, loss, accuracy):\n",
    "    \n",
    "    loss_ = sess.run(loss, feed_dict={x: xs_batch, y:ys_batch, keep_prob:1.0})\n",
    "    acc = sess.run(accuracy, feed_dict={x: xs_batch, y:ys_batch, keep_prob:1.0})\n",
    "    \n",
    "    print('Loss {} Accuracy {} %'.format(loss_, acc * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Convolutional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_conv_net(x, keep_prob):\n",
    "\n",
    "    #Creates a conv neural net\n",
    "    #: x: Placeholder with image data\n",
    "    #: keep_prob: Placeholder holds the keep probability after dropout\n",
    "    #: output is the logits\n",
    "\n",
    "    conv_stride = (1,1)\n",
    "    conv_ksize = (5, 5)\n",
    "    pool_stride = (2, 2)\n",
    "    pool_ksize = (2, 2)\n",
    "    \n",
    "    conv1_outputs = 32\n",
    "    conv2_outputs = 64\n",
    "    conv3_outputs = 128\n",
    "    full = 512\n",
    "    #conv_str, conv_ksize, pool_ksi, pool_str\n",
    "    conv1 = conv2d_pool(x, conv1_outputs, conv_stride, conv_ksize, pool_ksize, pool_stride)\n",
    "    conv2 = conv2d_pool(conv1, conv2_outputs, conv_stride, conv_ksize, pool_ksize, pool_stride)\n",
    "    conv3 = conv2d_pool(conv2, conv3_outputs, conv_stride, conv_ksize, pool_ksize, pool_stride)\n",
    "    \n",
    "    #flatten layer\n",
    "    conv3_flatten = flatten(conv3)\n",
    "    conv3_dropout = tf.nn.dropout(conv3_flatten, keep_prob)\n",
    "    \n",
    "    #fully connected layer\n",
    "    fully = fully_connected(conv3_dropout, full)\n",
    "    fully_drop = tf.nn.dropout(fully, keep_prob)\n",
    "    \n",
    "    #output\n",
    "    y_convolution = final_out(fully_drop, n_classes)\n",
    "    return y_convolution\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize our Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_classes = 10\n",
    "learning_rate = 2e-4\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "keep_probability = 0.5\n",
    "\n",
    "# Placeholders for inputs and outoputs\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3], name='inputs')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, n_classes], name='outputs')\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "\n",
    "#creating our model\n",
    "logits = create_conv_net(x, keep_prob)\n",
    "\n",
    "#naming variable\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# accuracy\n",
    "prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32), name='accuracy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model_path = './image_classification'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize training\n",
      "Epoch 1, CIFAR-10 Batch 1:\n",
      "Loss 2.216984272003174 Accuracy 15.000000596046448 %\n",
      "Epoch 1, CIFAR-10 Batch 2:\n",
      "Loss 2.10894775390625 Accuracy 32.50000178813934 %\n",
      "Epoch 1, CIFAR-10 Batch 3:\n",
      "Loss 1.8059405088424683 Accuracy 34.99999940395355 %\n",
      "Epoch 1, CIFAR-10 Batch 4:\n",
      "Loss 1.8079770803451538 Accuracy 25.0 %\n",
      "Epoch 1, CIFAR-10 Batch 5:\n",
      "Loss 1.861507534980774 Accuracy 27.500000596046448 %\n",
      "Epoch 2, CIFAR-10 Batch 1:\n",
      "Loss 1.933701992034912 Accuracy 37.5 %\n",
      "Epoch 2, CIFAR-10 Batch 2:\n",
      "Loss 1.9096662998199463 Accuracy 39.99999761581421 %\n",
      "Epoch 2, CIFAR-10 Batch 3:\n",
      "Loss 1.4683904647827148 Accuracy 44.999998807907104 %\n",
      "Epoch 2, CIFAR-10 Batch 4:\n",
      "Loss 1.588769555091858 Accuracy 40.00000357627869 %\n",
      "Epoch 2, CIFAR-10 Batch 5:\n",
      "Loss 1.6288187503814697 Accuracy 42.500001192092896 %\n",
      "Epoch 3, CIFAR-10 Batch 1:\n",
      "Loss 1.7551279067993164 Accuracy 44.999998807907104 %\n",
      "Epoch 3, CIFAR-10 Batch 2:\n",
      "Loss 1.8027806282043457 Accuracy 39.99999761581421 %\n",
      "Epoch 3, CIFAR-10 Batch 3:\n",
      "Loss 1.2793433666229248 Accuracy 57.499998807907104 %\n",
      "Epoch 3, CIFAR-10 Batch 4:\n",
      "Loss 1.4526993036270142 Accuracy 40.00000357627869 %\n",
      "Epoch 3, CIFAR-10 Batch 5:\n",
      "Loss 1.503373146057129 Accuracy 50.0 %\n",
      "Epoch 4, CIFAR-10 Batch 1:\n",
      "Loss 1.6155081987380981 Accuracy 42.500001192092896 %\n",
      "Epoch 4, CIFAR-10 Batch 2:\n",
      "Loss 1.6173194646835327 Accuracy 45.00000178813934 %\n",
      "Epoch 4, CIFAR-10 Batch 3:\n",
      "Loss 1.1712613105773926 Accuracy 57.499998807907104 %\n",
      "Epoch 4, CIFAR-10 Batch 4:\n",
      "Loss 1.4160641431808472 Accuracy 52.50000357627869 %\n",
      "Epoch 4, CIFAR-10 Batch 5:\n",
      "Loss 1.4314749240875244 Accuracy 55.000001192092896 %\n",
      "Epoch 5, CIFAR-10 Batch 1:\n",
      "Loss 1.498611569404602 Accuracy 47.49999940395355 %\n",
      "Epoch 5, CIFAR-10 Batch 2:\n",
      "Loss 1.5146448612213135 Accuracy 47.50000238418579 %\n",
      "Epoch 5, CIFAR-10 Batch 3:\n",
      "Loss 1.1109439134597778 Accuracy 62.5 %\n",
      "Epoch 5, CIFAR-10 Batch 4:\n",
      "Loss 1.2575693130493164 Accuracy 64.99999761581421 %\n",
      "Epoch 5, CIFAR-10 Batch 5:\n",
      "Loss 1.3428536653518677 Accuracy 50.0 %\n",
      "Epoch 6, CIFAR-10 Batch 1:\n",
      "Loss 1.3815072774887085 Accuracy 55.000001192092896 %\n",
      "Epoch 6, CIFAR-10 Batch 2:\n",
      "Loss 1.3760652542114258 Accuracy 50.0 %\n",
      "Epoch 6, CIFAR-10 Batch 3:\n",
      "Loss 1.0126209259033203 Accuracy 67.49999523162842 %\n",
      "Epoch 6, CIFAR-10 Batch 4:\n",
      "Loss 1.136228084564209 Accuracy 62.5 %\n",
      "Epoch 6, CIFAR-10 Batch 5:\n",
      "Loss 1.2809715270996094 Accuracy 55.000001192092896 %\n",
      "Epoch 7, CIFAR-10 Batch 1:\n",
      "Loss 1.273881435394287 Accuracy 60.00000238418579 %\n",
      "Epoch 7, CIFAR-10 Batch 2:\n",
      "Loss 1.2455213069915771 Accuracy 50.0 %\n",
      "Epoch 7, CIFAR-10 Batch 3:\n",
      "Loss 0.9132312536239624 Accuracy 72.50000238418579 %\n",
      "Epoch 7, CIFAR-10 Batch 4:\n",
      "Loss 1.0739418268203735 Accuracy 57.499998807907104 %\n",
      "Epoch 7, CIFAR-10 Batch 5:\n",
      "Loss 1.2176092863082886 Accuracy 57.50000476837158 %\n",
      "Epoch 8, CIFAR-10 Batch 1:\n",
      "Loss 1.212756872177124 Accuracy 64.99999761581421 %\n",
      "Epoch 8, CIFAR-10 Batch 2:\n",
      "Loss 1.1289677619934082 Accuracy 55.000001192092896 %\n",
      "Epoch 8, CIFAR-10 Batch 3:\n",
      "Loss 0.8414559364318848 Accuracy 72.50000238418579 %\n",
      "Epoch 8, CIFAR-10 Batch 4:\n",
      "Loss 0.9883661866188049 Accuracy 64.99999761581421 %\n",
      "Epoch 8, CIFAR-10 Batch 5:\n",
      "Loss 1.1720011234283447 Accuracy 55.000001192092896 %\n",
      "Epoch 9, CIFAR-10 Batch 1:\n",
      "Loss 1.1666053533554077 Accuracy 62.5 %\n",
      "Epoch 9, CIFAR-10 Batch 2:\n",
      "Loss 1.0447676181793213 Accuracy 64.99999761581421 %\n",
      "Epoch 9, CIFAR-10 Batch 3:\n",
      "Loss 0.7780095338821411 Accuracy 80.0000011920929 %\n",
      "Epoch 9, CIFAR-10 Batch 4:\n",
      "Loss 0.9319091439247131 Accuracy 60.00000238418579 %\n",
      "Epoch 9, CIFAR-10 Batch 5:\n",
      "Loss 1.1065654754638672 Accuracy 60.00000238418579 %\n",
      "Epoch 10, CIFAR-10 Batch 1:\n",
      "Loss 1.0838243961334229 Accuracy 64.99999761581421 %\n",
      "Epoch 10, CIFAR-10 Batch 2:\n",
      "Loss 0.9479392170906067 Accuracy 64.99999761581421 %\n",
      "Epoch 10, CIFAR-10 Batch 3:\n",
      "Loss 0.7213419079780579 Accuracy 80.0000011920929 %\n",
      "Epoch 10, CIFAR-10 Batch 4:\n",
      "Loss 0.8719271421432495 Accuracy 67.49999523162842 %\n",
      "Epoch 10, CIFAR-10 Batch 5:\n",
      "Loss 1.0068193674087524 Accuracy 60.00000238418579 %\n",
      "Epoch 11, CIFAR-10 Batch 1:\n",
      "Loss 1.009447455406189 Accuracy 64.99999761581421 %\n",
      "Epoch 11, CIFAR-10 Batch 2:\n",
      "Loss 0.8701658844947815 Accuracy 64.99999761581421 %\n",
      "Epoch 11, CIFAR-10 Batch 3:\n",
      "Loss 0.7000948786735535 Accuracy 77.50000357627869 %\n",
      "Epoch 11, CIFAR-10 Batch 4:\n",
      "Loss 0.8285501599311829 Accuracy 67.5000011920929 %\n",
      "Epoch 11, CIFAR-10 Batch 5:\n",
      "Loss 0.9425951242446899 Accuracy 67.50000715255737 %\n",
      "Epoch 12, CIFAR-10 Batch 1:\n",
      "Loss 0.9784651398658752 Accuracy 70.00000476837158 %\n",
      "Epoch 12, CIFAR-10 Batch 2:\n",
      "Loss 0.7998948693275452 Accuracy 67.5000011920929 %\n",
      "Epoch 12, CIFAR-10 Batch 3:\n",
      "Loss 0.6260983943939209 Accuracy 82.4999988079071 %\n",
      "Epoch 12, CIFAR-10 Batch 4:\n",
      "Loss 0.7705578804016113 Accuracy 75.0 %\n",
      "Epoch 12, CIFAR-10 Batch 5:\n",
      "Loss 0.8724715113639832 Accuracy 65.00000357627869 %\n",
      "Epoch 13, CIFAR-10 Batch 1:\n",
      "Loss 0.8614681959152222 Accuracy 72.50000238418579 %\n",
      "Epoch 13, CIFAR-10 Batch 2:\n",
      "Loss 0.7669003009796143 Accuracy 69.9999988079071 %\n",
      "Epoch 13, CIFAR-10 Batch 3:\n",
      "Loss 0.5704584121704102 Accuracy 82.4999988079071 %\n",
      "Epoch 13, CIFAR-10 Batch 4:\n",
      "Loss 0.7382308840751648 Accuracy 79.99999523162842 %\n",
      "Epoch 13, CIFAR-10 Batch 5:\n",
      "Loss 0.7831628918647766 Accuracy 72.50000238418579 %\n",
      "Epoch 14, CIFAR-10 Batch 1:\n",
      "Loss 0.8135246634483337 Accuracy 72.50000238418579 %\n",
      "Epoch 14, CIFAR-10 Batch 2:\n",
      "Loss 0.6856996417045593 Accuracy 72.50000238418579 %\n",
      "Epoch 14, CIFAR-10 Batch 3:\n",
      "Loss 0.554105818271637 Accuracy 82.4999988079071 %\n",
      "Epoch 14, CIFAR-10 Batch 4:\n",
      "Loss 0.6812003254890442 Accuracy 77.49999761581421 %\n",
      "Epoch 14, CIFAR-10 Batch 5:\n",
      "Loss 0.7535479664802551 Accuracy 72.50000238418579 %\n",
      "Epoch 15, CIFAR-10 Batch 1:\n",
      "Loss 0.7340055704116821 Accuracy 72.50000238418579 %\n",
      "Epoch 15, CIFAR-10 Batch 2:\n",
      "Loss 0.6422633528709412 Accuracy 72.50000238418579 %\n",
      "Epoch 15, CIFAR-10 Batch 3:\n",
      "Loss 0.5194315910339355 Accuracy 84.99999642372131 %\n",
      "Epoch 15, CIFAR-10 Batch 4:\n",
      "Loss 0.6506226062774658 Accuracy 82.50000476837158 %\n",
      "Epoch 15, CIFAR-10 Batch 5:\n",
      "Loss 0.7315526604652405 Accuracy 72.50000238418579 %\n",
      "Epoch 16, CIFAR-10 Batch 1:\n",
      "Loss 0.7103644609451294 Accuracy 72.50000238418579 %\n",
      "Epoch 16, CIFAR-10 Batch 2:\n",
      "Loss 0.5946492552757263 Accuracy 77.50000357627869 %\n",
      "Epoch 16, CIFAR-10 Batch 3:\n",
      "Loss 0.4630657434463501 Accuracy 85.00000834465027 %\n",
      "Epoch 16, CIFAR-10 Batch 4:\n",
      "Loss 0.6157876253128052 Accuracy 79.99999523162842 %\n",
      "Epoch 16, CIFAR-10 Batch 5:\n",
      "Loss 0.614874005317688 Accuracy 85.00000238418579 %\n",
      "Epoch 17, CIFAR-10 Batch 1:\n",
      "Loss 0.6538028717041016 Accuracy 75.0 %\n",
      "Epoch 17, CIFAR-10 Batch 2:\n",
      "Loss 0.5558086633682251 Accuracy 77.50000357627869 %\n",
      "Epoch 17, CIFAR-10 Batch 3:\n",
      "Loss 0.4464210867881775 Accuracy 90.00000357627869 %\n",
      "Epoch 17, CIFAR-10 Batch 4:\n",
      "Loss 0.5656468868255615 Accuracy 82.50000476837158 %\n",
      "Epoch 17, CIFAR-10 Batch 5:\n",
      "Loss 0.56919264793396 Accuracy 82.50000476837158 %\n",
      "Epoch 18, CIFAR-10 Batch 1:\n",
      "Loss 0.5859711766242981 Accuracy 77.49999761581421 %\n",
      "Epoch 18, CIFAR-10 Batch 2:\n",
      "Loss 0.49594563245773315 Accuracy 80.0000011920929 %\n",
      "Epoch 18, CIFAR-10 Batch 3:\n",
      "Loss 0.4248530864715576 Accuracy 92.5000011920929 %\n",
      "Epoch 18, CIFAR-10 Batch 4:\n",
      "Loss 0.5246231555938721 Accuracy 85.00000238418579 %\n",
      "Epoch 18, CIFAR-10 Batch 5:\n",
      "Loss 0.5162734985351562 Accuracy 92.49999523162842 %\n",
      "Epoch 19, CIFAR-10 Batch 1:\n",
      "Loss 0.5831084847450256 Accuracy 80.00000715255737 %\n",
      "Epoch 19, CIFAR-10 Batch 2:\n",
      "Loss 0.47867298126220703 Accuracy 82.50000476837158 %\n",
      "Epoch 19, CIFAR-10 Batch 3:\n",
      "Loss 0.3842812180519104 Accuracy 90.00000357627869 %\n",
      "Epoch 19, CIFAR-10 Batch 4:\n",
      "Loss 0.5191777348518372 Accuracy 87.50000596046448 %\n",
      "Epoch 19, CIFAR-10 Batch 5:\n",
      "Loss 0.48891595005989075 Accuracy 92.5000011920929 %\n",
      "Epoch 20, CIFAR-10 Batch 1:\n",
      "Loss 0.5383690595626831 Accuracy 80.00000715255737 %\n",
      "Epoch 20, CIFAR-10 Batch 2:\n",
      "Loss 0.4501352608203888 Accuracy 90.00000357627869 %\n",
      "Epoch 20, CIFAR-10 Batch 3:\n",
      "Loss 0.351485013961792 Accuracy 97.49999642372131 %\n",
      "Epoch 20, CIFAR-10 Batch 4:\n",
      "Loss 0.4733503460884094 Accuracy 87.50000596046448 %\n",
      "Epoch 20, CIFAR-10 Batch 5:\n",
      "Loss 0.4304924011230469 Accuracy 94.9999988079071 %\n",
      "Epoch 21, CIFAR-10 Batch 1:\n",
      "Loss 0.5112600922584534 Accuracy 80.00000715255737 %\n",
      "Epoch 21, CIFAR-10 Batch 2:\n",
      "Loss 0.4124380648136139 Accuracy 94.9999988079071 %\n",
      "Epoch 21, CIFAR-10 Batch 3:\n",
      "Loss 0.3412434160709381 Accuracy 94.9999988079071 %\n",
      "Epoch 21, CIFAR-10 Batch 4:\n",
      "Loss 0.44326573610305786 Accuracy 90.00000357627869 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, CIFAR-10 Batch 5:\n",
      "Loss 0.3795355558395386 Accuracy 94.9999988079071 %\n",
      "Epoch 22, CIFAR-10 Batch 1:\n",
      "Loss 0.49625641107559204 Accuracy 82.50000476837158 %\n",
      "Epoch 22, CIFAR-10 Batch 2:\n",
      "Loss 0.3796396851539612 Accuracy 92.5000011920929 %\n",
      "Epoch 22, CIFAR-10 Batch 3:\n",
      "Loss 0.3418569564819336 Accuracy 94.9999988079071 %\n",
      "Epoch 22, CIFAR-10 Batch 4:\n",
      "Loss 0.42240291833877563 Accuracy 90.00000357627869 %\n",
      "Epoch 22, CIFAR-10 Batch 5:\n",
      "Loss 0.35687360167503357 Accuracy 94.9999988079071 %\n",
      "Epoch 23, CIFAR-10 Batch 1:\n",
      "Loss 0.4432607591152191 Accuracy 85.00000238418579 %\n",
      "Epoch 23, CIFAR-10 Batch 2:\n",
      "Loss 0.38381144404411316 Accuracy 94.9999988079071 %\n",
      "Epoch 23, CIFAR-10 Batch 3:\n",
      "Loss 0.31639131903648376 Accuracy 94.9999988079071 %\n",
      "Epoch 23, CIFAR-10 Batch 4:\n",
      "Loss 0.41513851284980774 Accuracy 94.9999988079071 %\n",
      "Epoch 23, CIFAR-10 Batch 5:\n",
      "Loss 0.3241739273071289 Accuracy 94.9999988079071 %\n",
      "Epoch 24, CIFAR-10 Batch 1:\n",
      "Loss 0.38807421922683716 Accuracy 87.50000596046448 %\n",
      "Epoch 24, CIFAR-10 Batch 2:\n",
      "Loss 0.35299015045166016 Accuracy 94.9999988079071 %\n",
      "Epoch 24, CIFAR-10 Batch 3:\n",
      "Loss 0.2980230748653412 Accuracy 94.9999988079071 %\n",
      "Epoch 24, CIFAR-10 Batch 4:\n",
      "Loss 0.37935787439346313 Accuracy 90.00000357627869 %\n",
      "Epoch 24, CIFAR-10 Batch 5:\n",
      "Loss 0.29470306634902954 Accuracy 97.50000238418579 %\n",
      "Epoch 25, CIFAR-10 Batch 1:\n",
      "Loss 0.3853554129600525 Accuracy 89.99999761581421 %\n",
      "Epoch 25, CIFAR-10 Batch 2:\n",
      "Loss 0.2973034083843231 Accuracy 100.0 %\n",
      "Epoch 25, CIFAR-10 Batch 3:\n",
      "Loss 0.2622465193271637 Accuracy 97.49999642372131 %\n",
      "Epoch 25, CIFAR-10 Batch 4:\n",
      "Loss 0.3541123867034912 Accuracy 90.00000357627869 %\n",
      "Epoch 25, CIFAR-10 Batch 5:\n",
      "Loss 0.26114514470100403 Accuracy 100.0 %\n",
      "Epoch 26, CIFAR-10 Batch 1:\n",
      "Loss 0.3734138011932373 Accuracy 92.5000011920929 %\n",
      "Epoch 26, CIFAR-10 Batch 2:\n",
      "Loss 0.2899150252342224 Accuracy 100.0 %\n",
      "Epoch 26, CIFAR-10 Batch 3:\n",
      "Loss 0.25452977418899536 Accuracy 97.49999642372131 %\n",
      "Epoch 26, CIFAR-10 Batch 4:\n",
      "Loss 0.3325977027416229 Accuracy 94.9999988079071 %\n",
      "Epoch 26, CIFAR-10 Batch 5:\n",
      "Loss 0.26363223791122437 Accuracy 97.49999642372131 %\n",
      "Epoch 27, CIFAR-10 Batch 1:\n",
      "Loss 0.31639033555984497 Accuracy 97.49999642372131 %\n",
      "Epoch 27, CIFAR-10 Batch 2:\n",
      "Loss 0.2822645902633667 Accuracy 100.0 %\n",
      "Epoch 27, CIFAR-10 Batch 3:\n",
      "Loss 0.2549152970314026 Accuracy 97.49999642372131 %\n",
      "Epoch 27, CIFAR-10 Batch 4:\n",
      "Loss 0.32503876090049744 Accuracy 92.5000011920929 %\n",
      "Epoch 27, CIFAR-10 Batch 5:\n",
      "Loss 0.23496082425117493 Accuracy 97.49999642372131 %\n",
      "Epoch 28, CIFAR-10 Batch 1:\n",
      "Loss 0.2990402579307556 Accuracy 100.0 %\n",
      "Epoch 28, CIFAR-10 Batch 2:\n",
      "Loss 0.27798861265182495 Accuracy 97.49999642372131 %\n",
      "Epoch 28, CIFAR-10 Batch 3:\n",
      "Loss 0.24285224080085754 Accuracy 97.49999642372131 %\n",
      "Epoch 28, CIFAR-10 Batch 4:\n",
      "Loss 0.30376937985420227 Accuracy 94.9999988079071 %\n",
      "Epoch 28, CIFAR-10 Batch 5:\n",
      "Loss 0.23153428733348846 Accuracy 97.49999642372131 %\n",
      "Epoch 29, CIFAR-10 Batch 1:\n",
      "Loss 0.2662338316440582 Accuracy 100.0 %\n",
      "Epoch 29, CIFAR-10 Batch 2:\n",
      "Loss 0.22236648201942444 Accuracy 100.0 %\n",
      "Epoch 29, CIFAR-10 Batch 3:\n",
      "Loss 0.1999770849943161 Accuracy 100.0 %\n",
      "Epoch 29, CIFAR-10 Batch 4:\n",
      "Loss 0.27189651131629944 Accuracy 97.50000238418579 %\n",
      "Epoch 29, CIFAR-10 Batch 5:\n",
      "Loss 0.2056463360786438 Accuracy 100.0 %\n",
      "Epoch 30, CIFAR-10 Batch 1:\n",
      "Loss 0.2757694125175476 Accuracy 97.49999642372131 %\n",
      "Epoch 30, CIFAR-10 Batch 2:\n",
      "Loss 0.22387759387493134 Accuracy 100.0 %\n",
      "Epoch 30, CIFAR-10 Batch 3:\n",
      "Loss 0.18668979406356812 Accuracy 100.0 %\n",
      "Epoch 30, CIFAR-10 Batch 4:\n",
      "Loss 0.2685691714286804 Accuracy 92.5000011920929 %\n",
      "Epoch 30, CIFAR-10 Batch 5:\n",
      "Loss 0.19591844081878662 Accuracy 97.49999642372131 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Initialize training\")\n",
    "import time as tp\n",
    "start = tp.time()\n",
    "try:\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            n_batches = 5\n",
    "            for batch in range(1, n_batches + 1):\n",
    "                for xs_batch, ys_batch in helper.load_preprocess_training_batch(batch, batch_size):\n",
    "                    train_net(sess, optimizer, keep_probability, xs_batch, ys_batch)\n",
    "                print('Epoch {}, CIFAR-10 Batch {}:'.format(epoch + 1, batch))\n",
    "                print_stats(sess, xs_batch, ys_batch, loss, accuracy)\n",
    "        # Save Model\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, save_model_path)            \n",
    "except KeyboardInterrupt:\n",
    "    print ('Training Interrupted {} minutes'.format((tp.time() - start)//60))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.739131435751915\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_validation.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('inputs:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('outputs:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        #random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        #random_test_predictions = sess.run(\n",
    "            #tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            #feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        #helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
